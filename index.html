<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jinbo Wang 王锦波 </title> <meta name="author" content="Jinbo Wang"> <meta name="description" content="Ph.D. candidate in Computational Mathematics at Peking University. Research on LLM optimization and scaling laws. "> <meta name="keywords" content="LLM, pre-training, post-training, deep learning theory, scaling laws"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/me/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/me/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/me/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/me/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/me/assets/img/Giovanna.jpeg?v=c8fef2759f6347e26d3f52e42ff78f91"> <link rel="stylesheet" href="/me/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wang-jinbo.github.io/me/"> <script src="/me/assets/js/theme.js?v=561b4de9fbf7bf604dafd8b6fb3f1ae5"></script> <link defer rel="stylesheet" href="/me/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/me/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/me/publications/">Publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Jinbo Wang 王锦波 </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/me/assets/img/prof_pic_new-480.webp 480w,/me/assets/img/prof_pic_new-800.webp 800w,/me/assets/img/prof_pic_new-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/me/assets/img/prof_pic_new.png?v=b4d879be7040dbe9aad30d870fd372ce" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic_new.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>Hi! My name is <strong>Jinbo Wang</strong> (<strong>王锦波</strong> in Chinese).</p> <p>I am a 4th-year Ph.D. candidate in Computational Mathematics at <a href="https://www.math.pku.edu.cn/" rel="external nofollow noopener" target="_blank">School of Mathematical Sciences</a>, <a href="https://www.pku.edu.cn/" rel="external nofollow noopener" target="_blank">Peking University</a>, fortunate to be co-advised by <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Prof. Weinan E</a> since 2022 and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Prof. Lei Wu</a> since 2025.</p> <p>My research bridges deep learning theory and large-scale LLM training. I study principled optimization methods, grounded in loss landscape geometry and scaling law analysis, to make LLM pre-training and post-training faster and more efficient. </p> <p>I am expected to graduate in 2027 and will be on the job market. Feel free to reach out!</p> <p><a href="mailto:wangjinbo@ustc.edu"><i class="fa-solid fa-envelope"></i> Email</a> | <a href="https://linkedin.com/in/jinbo-wang-95538a191/" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i> LinkedIn</a> | <a href="https://scholar.google.com/citations?user=eHr_yNgAAAAJ" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i> Google Scholar</a><i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="bottom" data-html="true" data-content="¹ Please contact me to get my CV.&lt;br&gt;² Google Scholar struggles to parse my advisor's surname 'E', causing misattributed articles with * and missing citation counts."> </i></p> </div> <h2>Education</h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">2022 ~ present</th> <td> <span class="font-weight-bold"> School of Mathematical Sciences, Peking University </span> <br> <span class="font-italic">Ph.D. Candidate in Computational Mathematics</span> <br> <span class="text-muted">China’s Industrial Bank Scholarship, Merit Student, <strong>School Scholarship</strong>, Alpha2Fund Scholarship, Third Class Scholarship </span> </td> </tr> <tr> <th scope="row" style="width: 20%">2018 ~ 2022</th> <td> <span class="font-weight-bold"> School of Mathematical Sciences, University of Science and Technology of China </span> <br> <span class="font-italic">B.S. in Computational and Applied Mathematics</span> <br> <span class="text-muted">First Class Scholarship, <strong>National Scholarship (2/129)</strong>, Third Class Scholarship </span> </td> </tr> </table> </div> </div> <h2> <a href="/me/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Tencent HY</abbr> </div> <div id="huang2026stabilizing" class="col-sm-9"> <div class="title">Stabilizing RLVR via Token-level Gradient Diagnosis and Layerwise Clipping</div> <div class="author"> <a href="https://carlanlark.github.io/" rel="external nofollow noopener" target="_blank">Guanhua Huang<sup>*</sup></a>, Tingqiang Xu<sup>*</sup>, <em>Jinbo Wang<sup>*</sup></em>, Guangming Sheng, Siheng Li, Evander Yang, Kejiao Li, Yunxiang Li, Zenan Xu, Qi Yi, Kyrierl Deng, Ziyuan Nan, Yuhao Jiang, Chenchen Zhang, Taiqiang Wu, Feiyuan Zhang, Junhao Wang, Bo Zhou, Alex Chen, Di Wang, and <a href="https://ysymyth.github.io/" rel="external nofollow noopener" target="_blank">Shunyu Yao</a> </div> <div class="periodical"> <em>Tencent HunYuan Research Blog</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a href="https://hy.tencent.com/research/100015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://mp.weixin.qq.com/s/Q3iiBg7ODdKgbMwdEfrNGQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Wechat</a> </div> <div class="tldr hidden"> <p>We present GradLoc: transferring RLVR training collapse diagnostic from black-box heuristics to white-box fine-grained token localization via a distributed bisection, paired with layerwise clipping as a practical safeguard.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2026fastcatch" class="col-sm-9"> <div class="title">Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws</div> <div class="author"> <em>Jinbo Wang<sup>*</sup></em>, <a href="https://libinghui0000.github.io/" rel="external nofollow noopener" target="_blank">Binghui Li<sup>*</sup></a>, <a href="https://zzp1012.github.io/" rel="external nofollow noopener" target="_blank">Zhanpeng Zhou</a>, <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang</a>, Yuxuan Sun, Jiaqi Zhang, Xunliang Cai, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=PXWgzUkVwo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We study batch size schedule: uncover optimal batch size schedule, fast catch-up effect and later switch strategy from Functional Scaling Laws (FSL) theoretical framework, and bring our insights to LLM pre-training.</p> </div> <div class="abstract hidden"> <p>Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the <strong>functional scaling law (FSL)</strong> framework introduced in <a href="https://arxiv.org/abs/2509.19189" rel="external nofollow noopener" target="_blank">Li et al. (2025a)</a> provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism—the <strong>fast catch-up</strong> effect—which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that <em>large batches can be safely deferred to late training</em> without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments—covering both Dense and MoE architectures with up to <strong>1.1B</strong> parameters and <strong>1T</strong> tokens—validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> </div> <div id="wang2026gradpower" class="col-sm-9"> <div class="title">GradPower: Powering Gradients for Faster Language Model Pre-Training</div> <div class="author"> <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang<sup>*</sup></a>, <em>Jinbo Wang<sup>*</sup></em>, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Weinan E</a>, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>arXiv preprint</em>, ICML 2026 submission, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2505.24275" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We propose GradPower, a lightweight sign-power gradient transformation, to accelerate LLM pre-training.</p> </div> <div class="abstract hidden"> <p>We propose <strong>GradPower</strong>, a lightweight gradient-transformation technique for accelerating language model pre-training. Given a gradient vector $\boldsymbol{g}=(g_{i})_{i}$, GradPower first applies the elementwise <code class="language-plaintext highlighter-rouge">sign-power</code> transformation: $ \varphi_p(\boldsymbol{g}) = \left({\rm sign}(g_i)|g_i|^p\right)_{i} $ for a fixed $p\gt;0$, and then feeds the transformed gradient into a base optimizer. Notably, GradPower requires only a <strong>single-line code change</strong> and no modifications to the base optimizer’s internal logic, including the hyperparameters. When applied to AdamW (termed <strong>AdamWPower</strong>), GradPower consistently achieves lower terminal loss across diverse architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4, OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The most pronounced gains are observed when training modern mixture-of-experts models with warmup-stable-decay schedules. GradPower also integrates seamlessly with other state-of-the-art optimizers, such as Muon, yielding further improvements. Finally, we provide theoretical analyses that reveal the underlying mechanism of GradPower and highlight the influence of gradient noise.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="wang2025sharpness" class="col-sm-9"> <div class="title">The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training</div> <div class="author"> <em>Jinbo Wang<sup>*</sup></em>, <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang<sup>*</sup></a>, <a href="https://zzp1012.github.io/" rel="external nofollow noopener" target="_blank">Zhanpeng Zhou<sup>*</sup></a>, <a href="https://thinklab.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank">Junchi Yan</a>, <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Weinan E</a>, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v267/wang25dl.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We uncover a persistent Hessian heterogeneity in Transformer and turn it into a practical blockwise learning rate strategy via Edge of Stability (EoS) theory. Our algorithm achieves lower terminal loss and faster pre-training across settings.</p> </div> <div class="abstract hidden"> <p>Transformers have become the cornerstone of modern AI. Unlike traditional architectures, transformers exhibit a distinctive characteristic: diverse types of building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feed-forward networks, work collaboratively. Understanding the disparities and interactions among these blocks is therefore important. In this paper, we uncover a clear <strong>sharpness disparity</strong> across these blocks, which intriguingly emerges early in training and persists throughout the training process. Building on this insight, we propose a novel <strong>Blockwise Learning Rate (LR)</strong> strategy to accelerate large language model (LLM) pre-training. Specifically, by integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\times$ speedup compared to vanilla AdamW. This improvement is demonstrated across GPT-2 and LLaMA models, with model sizes ranging from 0.12B to 1.1B and datasets including OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of Adam, achieving a combined $2\times$ speedup and $2\times$ memory savings. These results underscore the potential of leveraging the sharpness disparity principle to improve LLM training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2024improving" class="col-sm-9"> <div class="title">Improving Generalization and Convergence by Enhancing Implicit Regularization</div> <div class="author"> <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang</a>, <em>Jinbo Wang</em>, Haotian He, Zilin Wang, <a href="https://carlanlark.github.io/" rel="external nofollow noopener" target="_blank">Guanhua Huang</a>, Feiyu Xiong, Zhiyu Li, <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Weinan E</a>, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://papers.nips.cc/paper_files/paper/2024/hash/d712c8625fd97424c9744019b28dca21-Abstract-Conference.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>Our algorithm enhances implicit regularization to reach flatter minima faster, improving generalization while maintaining stable optimization across vision and LLM setups.</p> </div> <div class="abstract hidden"> <p>In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence. Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions. We show that IRE can be practically incorporated with <em>generic base optimizers</em> without introducing significant computational overload. Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also achieves a $2\times$ <em>speed-up</em> compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in Sharpness-aware Minimization (SAM).</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Jinbo Wang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/me/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/me/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/me/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/me/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/me/assets/js/common.js?v=48797841c500c92f60ff2a0dcba10e65"></script> <script defer src="/me/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/me/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/me/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/me/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/me/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>