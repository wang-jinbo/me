<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Jinbo Wang 王锦波 </title> <meta name="author" content="Jinbo Wang"> <meta name="description" content="publications by categories in reversed chronological order."> <meta name="keywords" content="LLM, pre-training, post-training, deep learning theory, scaling laws"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/me/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/me/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/me/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/me/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/me/assets/img/Giovanna.jpeg?v=c8fef2759f6347e26d3f52e42ff78f91"> <link rel="stylesheet" href="/me/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wang-jinbo.github.io/me/publications/"> <script src="/me/assets/js/theme.js?v=561b4de9fbf7bf604dafd8b6fb3f1ae5"></script> <link defer rel="stylesheet" href="/me/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/me/"> Jinbo Wang 王锦波 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/me/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/me/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Tencent HY</abbr> </div> <div id="huang2026stabilizing" class="col-sm-9"> <div class="title">Stabilizing RLVR via Token-level Gradient Diagnosis and Layerwise Clipping</div> <div class="author"> <a href="https://carlanlark.github.io/" rel="external nofollow noopener" target="_blank">Guanhua Huang<sup>*</sup></a>, Tingqiang Xu<sup>*</sup>, <em>Jinbo Wang<sup>*</sup></em>, Guangming Sheng, Siheng Li, Evander Yang, Kejiao Li, Yunxiang Li, Zenan Xu, Qi Yi, Kyrierl Deng, Ziyuan Nan, Yuhao Jiang, Chenchen Zhang, Taiqiang Wu, Feiyuan Zhang, Junhao Wang, Bo Zhou, Alex Chen, Di Wang, and <a href="https://ysymyth.github.io/" rel="external nofollow noopener" target="_blank">Shunyu Yao</a> </div> <div class="periodical"> <em>Tencent HunYuan Research Blog</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a href="https://hy.tencent.com/research/100015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We present GradLoc: transferring RLVR training collapse diagnostic from black-box heuristics to white-box fine-grained token localization via a distributed bisection, paired with layerwise clipping as a practical safeguard.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2026fastcatch" class="col-sm-9"> <div class="title">Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws</div> <div class="author"> <em>Jinbo Wang<sup>*</sup></em>, <a href="https://libinghui0000.github.io/" rel="external nofollow noopener" target="_blank">Binghui Li<sup>*</sup></a>, <a href="https://zzp1012.github.io/" rel="external nofollow noopener" target="_blank">Zhanpeng Zhou</a>, <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang</a>, Yuxuan Sun, Jiaqi Zhang, Xunliang Cai, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=PXWgzUkVwo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We study batch size schedule: uncover optimal batch size schedule, fast catch-up effect and later switch strategy from Functional Scaling Laws (FSL) theoretical framework, and bring our insights to LLM pre-training.</p> </div> <div class="abstract hidden"> <p>Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the <strong>functional scaling law (FSL)</strong> framework introduced in <a href="https://arxiv.org/abs/2509.19189" rel="external nofollow noopener" target="_blank">Li et al. (2025a)</a> provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism—the <strong>fast catch-up</strong> effect—which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that <em>large batches can be safely deferred to late training</em> without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments—covering both Dense and MoE architectures with up to <strong>1.1B</strong> parameters and <strong>1T</strong> tokens—validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preparation</abbr> </div> <div id="li2026scaling" class="col-sm-9"> <div class="title">Scaling-Law Analysis of SignSGD: From Feature-Space Linear Regression to LLM Pre-training</div> <div class="author"> <a href="https://libinghui0000.github.io/" rel="external nofollow noopener" target="_blank">Binghui Li<sup>*</sup></a>, Jianan Wang<sup>*</sup>, <em>Jinbo Wang<sup>*</sup></em>, <a href="https://leanwang326.github.io/.github.io/" rel="external nofollow noopener" target="_blank">Lean Wang<sup>*</sup></a>, Zilin Wang<sup>*</sup>, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>Preparation</em>, ICML 2026 submission , 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> </div> <div class="tldr hidden"> <p>We develop a scaling law analysis for SignSGD, to understand why it outperforms SGD in large-scale training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preparation</abbr> </div> <div id="anonymous2026swemutation" class="col-sm-9"> <div class="title">SWE-Mutation: Can LLMs Generate Reliable Test Suites in Software Engineering?</div> <div class="author"> Yuxuan Sun and others </div> <div class="periodical"> <em>Preparation</em>, ACL 2026 submission , 2026 </div> <div class="periodical"> Sixth author </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preparation</abbr> </div> <div id="zhou2026how" class="col-sm-9"> <div class="title">How Does Local Landscape Geometry Evolve in Language Model Pre-Training?</div> <div class="author"> <a href="https://zzp1012.github.io/" rel="external nofollow noopener" target="_blank">Zhanpeng Zhou</a>, Yuhan Sun, <a href="https://bingrui-li.github.io/" rel="external nofollow noopener" target="_blank">Bingrui Li</a>, <em>Jinbo Wang</em>, Huaijin Wu, <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a>, and <a href="https://thinklab.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank">Junchi Yan</a> </div> <div class="periodical"> <em>Preparation</em>, ICML 2026 submission , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="tldr hidden"> <p>We study how loss landscape geometry evolves during LLM pre-training, explaining learning-rate warmup and yielding batch-size scheduling that substantially improve data efficiency.</p> </div> <div class="abstract hidden"> <p>The scale and expense of pre-training language models make efficient hyperparameter tuning essential, yet a principled guidance is still missing. Recent work shows that the geometry of loss landscape shapes training dynamics of neural networks and further informs hyperparameter choices. In this work, we analyze language model pre-training dynamics from a local landscape geometry perspective. Our study reveals two distinct phases. In the early phase, sharpness of the local landscape is initially high, leading to instability and loss plateaus under large learning rates (LRs). Later, the landscape shifts from sharp to flatter regions. This dynamic explains the necessity of LR warmup and further suggests that larger peak LRs require proportionally longer warmup periods. In the late phase, the local landscape is governed by the gradient noise scale. Through diffusion-limit analysis, we prove a depth–flatness trade-off: high noise from smaller batches widens the loss basin, whereas reduced noise from larger batches deepens it. This theory motivates a dynamic batch-size (BS) scheduler that begins with a small BS and increases it late in training. Together, we provide a unified account of loss landscape evolution, which translates into actionable tuning strategies for large-scale pre-training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> </div> <div id="wang2026gradpower" class="col-sm-9"> <div class="title">GradPower: Powering Gradients for Faster Language Model Pre-Training</div> <div class="author"> <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang<sup>*</sup></a>, <em>Jinbo Wang<sup>*</sup></em>, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Weinan E</a>, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>arXiv preprint</em>, ICML 2026 submission , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2505.24275" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We propose GradPower, a lightweight sign-power gradient transformation, to accelerate LLM pre-training.</p> </div> <div class="abstract hidden"> <p>We propose <strong>GradPower</strong>, a lightweight gradient-transformation technique for accelerating language model pre-training. Given a gradient vector $\boldsymbol{g}=(g_{i})_{i}$, GradPower first applies the elementwise <code class="language-plaintext highlighter-rouge">sign-power</code> transformation: $ \varphi_p(\boldsymbol{g}) = \left({\rm sign}(g_i)|g_i|^p\right)_{i} $ for a fixed $p\gt;0$, and then feeds the transformed gradient into a base optimizer. Notably, GradPower requires only a <strong>single-line code change</strong> and no modifications to the base optimizer’s internal logic, including the hyperparameters. When applied to AdamW (termed <strong>AdamWPower</strong>), GradPower consistently achieves lower terminal loss across diverse architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4, OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The most pronounced gains are observed when training modern mixture-of-experts models with warmup-stable-decay schedules. GradPower also integrates seamlessly with other state-of-the-art optimizers, such as Muon, yielding further improvements. Finally, we provide theoretical analyses that reveal the underlying mechanism of GradPower and highlight the influence of gradient noise.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="wang2025sharpness" class="col-sm-9"> <div class="title">The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training</div> <div class="author"> <em>Jinbo Wang<sup>*</sup></em>, <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang<sup>*</sup></a>, <a href="https://zzp1012.github.io/" rel="external nofollow noopener" target="_blank">Zhanpeng Zhou<sup>*</sup></a>, <a href="https://thinklab.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank">Junchi Yan</a>, <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Weinan E</a>, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v267/wang25dl.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We uncover a persistent Hessian heterogeneity in Transformer and turn it into a practical blockwise learning rate strategy via Edge of Stability (EoS) theory. Our algorithm achieves lower terminal loss and faster pre-training across settings.</p> </div> <div class="abstract hidden"> <p>Transformers have become the cornerstone of modern AI. Unlike traditional architectures, transformers exhibit a distinctive characteristic: diverse types of building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feed-forward networks, work collaboratively. Understanding the disparities and interactions among these blocks is therefore important. In this paper, we uncover a clear <strong>sharpness disparity</strong> across these blocks, which intriguingly emerges early in training and persists throughout the training process. Building on this insight, we propose a novel <strong>Blockwise Learning Rate (LR)</strong> strategy to accelerate large language model (LLM) pre-training. Specifically, by integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\times$ speedup compared to vanilla AdamW. This improvement is demonstrated across GPT-2 and LLaMA models, with model sizes ranging from 0.12B to 1.1B and datasets including OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of Adam, achieving a combined $2\times$ speedup and $2\times$ memory savings. These results underscore the potential of leveraging the sharpness disparity principle to improve LLM training.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2024improving" class="col-sm-9"> <div class="title">Improving Generalization and Convergence by Enhancing Implicit Regularization</div> <div class="author"> <a href="https://wmz9.github.io/" rel="external nofollow noopener" target="_blank">Mingze Wang</a>, <em>Jinbo Wang</em>, Haotian He, Zilin Wang, <a href="https://carlanlark.github.io/" rel="external nofollow noopener" target="_blank">Guanhua Huang</a>, Feiyu Xiong, Zhiyu Li, <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Weinan E</a>, and <a href="https://leiwu0.github.io/" rel="external nofollow noopener" target="_blank">Lei Wu</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.52202/079017-3770" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>Our algorithm enhances implicit regularization to reach flatter minima faster, improving generalization while maintaining stable optimization across vision and LLM setups.</p> </div> <div class="abstract hidden"> <p>In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence. Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions. We show that IRE can be practically incorporated with <em>generic base optimizers</em> without introducing significant computational overload. Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also achieves a $2\times$ <em>speed-up</em> compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in Sharpness-aware Minimization (SAM).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JML</abbr> </div> <div id="yang2024memory3" class="col-sm-9"> <div class="title">Memory³: Language Modeling with Explicit Memory</div> <div class="author"> Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, <em>Jinbo Wang</em>, Zeyun Tang, <a href="https://ki-seki.github.io/" rel="external nofollow noopener" target="_blank">Shichao Song</a>, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and <a href="https://web.math.princeton.edu/~weinan/" rel="external nofollow noopener" target="_blank">Weinan E</a> </div> <div class="periodical"> <em>Journal of Machine Learning</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2407.01178" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We propose Memory³: Externalizes knowledge into explicit memory, reducing reliance on parameters and improving efficiency toward separating reasoning vs. memory in language modeling.</p> </div> <div class="abstract hidden"> <p>The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining “abstract knowledge”. As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named Memory³, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICONIP</abbr> </div> <div id="min2023exploring" class="col-sm-9"> <div class="title">Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study</div> <div class="author"> Zeping Min and <em>Jinbo Wang</em> </div> <div class="periodical"> <em>In International Conference on Neural Information Processing (ICONIP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-981-99-8181-6_6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="tldr hidden"> <p>We explore the integration of LLMs into automatic speech recognition (ASR) systems to improve accuracy.</p> </div> <div class="abstract hidden"> <p>This paper explores the integration of Large Language Models (LLMs) into Automatic Speech Recognition (ASR) systems to improve transcription accuracy. The increasing sophistication of LLMs, with their in-context learning capabilities and instruction-following behavior, has drawn significant attention in the field of Natural Language Processing (NLP). Our primary focus is to investigate the potential of using an LLM’s in-context learning capabilities to enhance the performance of ASR systems, which currently face challenges such as ambient noise, speaker accents, and complex linguistic contexts. We designed a study using the Aishell-1 and LibriSpeech datasets, with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities. Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM’s in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the limitations of LLMs in speech applications. This paper provides a detailed overview of these experiments, their results, and implications, establishing that using LLMs’ in-context learning capabilities to correct potential errors in speech recognition transcriptions is still a challenging task at the current stage.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Jinbo Wang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/me/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/me/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/me/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/me/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/me/assets/js/common.js?v=48797841c500c92f60ff2a0dcba10e65"></script> <script defer src="/me/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/me/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/me/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/me/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/me/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>